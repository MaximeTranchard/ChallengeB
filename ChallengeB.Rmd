---
title: "ChallengeB"
author: "Emma Gaillat"
date: "30 novembre 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Task 1B - Predicting house prices in Ames, Iowa (continued)

## Step 1 - Choose a ML technique : non-parametric kernel estimation, random forests, etc. . . Give a brief intuition of how it works. (1 points)

# Machine learning technique: Random Forest: modeling

Like gradient boosted trees, random forests are another form of ensemble model. That is, they use lots of simpler models (decision trees, again) and combine them to make a single better model. Rather than running the same model iteratively, random forests run lots of separate models in parallel, each on a randomly chosen subset of the data, with a randomly chosen subset of features. Then the final decision tree makes predictions by aggregating the results from the individual models.


##Step 2 - Train the chosen technique on the training data. Hint : packages np for non-parametric regressions, randomForest for random forests. Don’t use the variable Id as a feature. (2 points)

CORRECTION CHALLENGE A

```{r housing-init, echo = TRUE}
load.libraries <- c('tidyverse', 'knitr')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependencies = TRUE, repos = "https://cloud.r-project.org")
sapply(load.libraries, require, character = TRUE)

```
```{r housing-step1-sol, echo = TRUE}

train <- read_csv(file = "C:/Users/Emma/Documents/Challenge B/train.csv")
test <- read_csv(file = "C:/Users/Emma/Documents/Challenge B/test.csv")

```
```{r structure}
dim(train)
```
```{r housing-step3-sol, echo= TRUE}
class(train$SalePrice)
train 
```
```{r housing-step5-sol, echo= TRUE}
summary(train) #that is the minimum required, gets +0.75
```
```{r housing-step6-sol, echo= TRUE}
hist(train$YearBuilt)
hist(train$LotArea)
hist(train$`1stFlrSF`)

ggplot(train)

par(mfrow = c(1,3))
ggplot(train) + geom_histogram(mapping = aes(x = YearBuilt))
ggplot(train) + geom_histogram(mapping = aes(x = LotArea))
ggplot(train) + geom_histogram(mapping = aes(x = `1stFlrSF`))

```
```{r missing data, echo= TRUE}
head(train) 

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

```
```{r missing data_plot}

plot_Missing <- function(data_in, title = NULL){
  temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
  temp_df <- temp_df[,order(colSums(temp_df))]
  data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
  data_temp$m <- as.vector(as.matrix(temp_df))
  data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
  ggplot(data_temp) + geom_tile(aes(x=x, y=y, fill=factor(m))) + scale_fill_manual(values=c("white", "black"), name="Missing\n(0=Yes, 1=No)") + theme_light() + ylab("") + xlab("") + ggtitle(title)
}


plot_Missing(train[,colSums(is.na(train)) > 0])
```
```{r missing data 2, echo= TRUE}
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))
```
```{r missing data 3, echo= TRUE}

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
```
```{r housing-step8-sol}
cat("The number of duplicated rows are", nrow(train) - nrow(unique(train)))

```
```{r housing-step9-sol, echo = TRUE}
cat_var <- train %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist


train %>% mutate_at(.cols = cat_var, .funs = as.factor)

```
```{r housing-step10-sol, echo = TRUE}
lm_model_1 <- lm(SalePrice ~ ., data= train)
summary(lm_model_1)


sum_lm_model_1 <- summary(lm_model_1)$coefficients 
class(sum_lm_model_1)
significant.vars <- row.names(sum_lm_model_1[sum_lm_model_1[,4] <= 0.01,]) 

lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
summary(lm_model_2)

```
```{r housing-step11-sol, echo = TRUE}

prediction <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
write.csv(x = prediction, file = "predictions.csv", na = "NA", quote = FALSE, row.names = FALSE)
```

REPONSE

```{r randomforest-step2, echo = TRUE}
install.packages("randomForest")
??randomForest```
require(randomForest)

```


##Step 3 - Make predictions on the test data, and compare them to the predictions of a linear regression of your choice. (2 points)

```{r prediction-step 3}
installed.packages("randomForest")
require(randomForest)
predict(object, data=test, type="response",
  norm.votes=TRUE, predict.all=FALSE, proximity=FALSE, nodes=FALSE)
#je sais pas quoi mettre à la place de "object", je pensais mettre Id mais il reconnaît pas
```



###Task 2B - Overfitting in Machine Learning (continued)

CORRECTION CHALLENGE A

```{r overfit, echo = FALSE, eval = FALSE, include = FALSE}
rm(list = ls())

library(tidyverse)

set.seed(1)
Nsim <- 150
b <- c(0,1)
x0 <- rep(1, Nsim)
x1 <- rnorm(n = Nsim)

X <- cbind(x0, x1^3)
y.true <- X %*% b

eps <- rnorm(n = Nsim)
y <- X %*% b + eps

df <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value)

ggplot(df) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true))

training.index <- createDataPartition(y = y, times = 1, p = 0.8)
df <- df %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "training", "test"))

training <- df %>% filter(which.data == "training")
test <- df %>% filter(which.data == "test")


lm.fit <- lm(y ~ x, data = training)
summary(lm.fit)

df <- df %>% mutate(y.lm = predict(object = lm.fit, newdata = df))
training <- training %>% mutate(y.lm = predict(object = lm.fit))
```
```{r step3-sol, echo = TRUE, eval = TRUE, include = TRUE}
rm(list = ls())
library(tidyverse)
library(np)
library(caret)
set.seed(1) 
Nsim <- 150 
b <- c(0,1) 
x0 <- rep(1, Nsim) 
x1 <- rnorm(n = Nsim)

X <- cbind(x0, x1^3)
y.true <- X %*% b

eps <- rnorm(n = Nsim) 
y <- X %*% b + eps 

df <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value) 
```
```{r overfit-step4, echo = FALSE, fig.cap = "Step 4 - Scatterplot x - y"}
ggplot(df) + geom_point(mapping = aes(x = x, y = y))

```
```{r overfit-step5, echo = FALSE, fig.cap = "Step 5 - True regression line"}
ggplot(df) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true))

```
```{r overfit-step6-sol, echo = TRUE}
training.index <- createDataPartition(y = y, times = 1, p = 0.8) 
df <- df %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "training", "test")) 

training <- df %>% filter(which.data == "training") sub-table
test <- df %>% filter(which.data == "test")

```
```{r overfit-step7-sol, echo = TRUE}
lm.fit <- lm(y ~ x, data = training) 
summary(lm.fit)

```
```{r overfit-step8-sol, echo = TRUE}
training <- training %>% mutate(y.lm = predict(object = lm.fit))
ggplot(training) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.lm), color = "orange")

```
```{r overfit-step6, echo = FALSE, fig.cap = "Step 6 - Training and test data"}
ggplot(df) + geom_point(mapping = aes(x = x, y = y, color = which.data))
```
```{r overfit-step8, echo = FALSE, fig.cap = "Step 8 - Linear regression line"}
ggplot(training) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.lm), color = "orange")
```

#Step 1 - Estimate a low-flexibility local linear model on the training data. For that, you can use function npreg the package np. Choose ll for the method (local linear), and a bandwidth of 0.5; Call this model ll.fit.lowflex

```{r TaskBstep1 echo = TRUE}
install.packages("np")
??np
```

```{r TaskBstep1bis echo = TRUE}
ll.fit.lowflex <- npreg(lowflexibility ~ ...,
                        + regtype = "ll",
                        + bwmethod = "cv.aic",
                        + gradients = TRUE,
                        + data = training)
summary ll.fit.lowflex
```

#Step 2 - Estimate a high-flexibility local linear model on the training data. For that, you can use function npreg the package np. Choose ll for the method (local linear), and a bandwidth of 0.01; Call this model ll.fit.highflex

#Step 3 - Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex, on only the training data. See Figure 1.

#Step 4 - Between the two models, which predictions are more variable? Which predictions have the least bias?

#Step 5 - Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex now using the test data. Which predictions are more variable? What happened to the bias of the least biased model? Now let’s see what happens to the overall error rate, that is the mean square error. 

#Step 6 - Create a vector of bandwidth going from 0.01 to 0.5 with a step of 0.001

#Step 7 - Estimate a local linear model y ~ x on the training data with each bandwidth.

#Step 8 - Compute for each bandwidth the MSE on the training data.

#Step 9 - Compute for each bandwidth the MSE on the test data.

#Step 10 - Draw on the same plot how the MSE on training data, and test data, change when the bandwidth increases. Conclude.

###Task 3B - Privacy regulation compliance in France

#Step 1 - Import the CNIL dataset from the Open Data Portal. (1 point)

```{r task3step1}
CIL <- read.csv(file= "C:/Users/Emma/Documents/Challenge B/OpenCNIL_Organismes_avec_CIL_VD_20171115.csv")

```

#Step 2 - Show a (nice) table with the number of organizations that has nominated a CNIL per department. HINT : A department in France is uniquely identified by the first two digits of the postcode. (1 point)

#Step 3 - Merge the information from the SIREN dataset into the CNIL data. Explain the method you use. HINT : In the SIREN dataset, there are some rows that refer to the same SIREN number, use the most up to date information about each company. (2 points)

#Step 4 - Plot the histogram of the size of the companies that nominated a CIL. Comment. (1 points)

